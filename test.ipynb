{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10558009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fe5936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "época 0, perda: 2.7279\n",
      "época 10, perda: 0.4855\n",
      "época 20, perda: 0.0280\n",
      "época 30, perda: 0.0055\n",
      "época 40, perda: 0.0025\n",
      "época 50, perda: 0.0017\n",
      "época 60, perda: 0.0014\n",
      "época 70, perda: 0.0012\n",
      "época 80, perda: 0.0011\n",
      "época 90, perda: 0.0010\n",
      "tque sim!? tudo bem? espero que sim!m!s!ero que sim\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Texto de exemplo\n",
    "texto = \"oi tudo bem? espero que sim!\"\n",
    "chars = sorted(set(texto))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "# Convertendo para tensores\n",
    "def texto_para_tensor(texto):\n",
    "    return torch.tensor([char2idx[c] for c in texto], dtype=torch.long)\n",
    "\n",
    "seq = texto_para_tensor(texto)\n",
    "\n",
    "# Modelo LSTM\n",
    "class LSTMTexto(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.embed(input)\n",
    "        out, hidden = self.lstm(emb.view(len(input), 1, -1), hidden)\n",
    "        out = self.fc(out.view(len(input), -1))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, hidden_dim), torch.zeros(1, 1, hidden_dim))\n",
    "\n",
    "# Treinamento\n",
    "hidden_dim = 64\n",
    "modelo = LSTMTexto(vocab_size, hidden_dim, 1)\n",
    "criterio = nn.CrossEntropyLoss()\n",
    "otimizador = torch.optim.Adam(modelo.parameters(), lr=0.01)\n",
    "\n",
    "for epoca in range(100):\n",
    "    hidden = modelo.init_hidden()\n",
    "    input_seq = seq[:-1]\n",
    "    target_seq = seq[1:]\n",
    "    \n",
    "    output, hidden = modelo(input_seq, hidden)\n",
    "    perda = criterio(output, target_seq)\n",
    "\n",
    "    otimizador.zero_grad()\n",
    "    perda.backward()\n",
    "    otimizador.step()\n",
    "\n",
    "    if epoca % 10 == 0:\n",
    "        print(f\"época {epoca}, perda: {perda.item():.4f}\")\n",
    "\n",
    "# Geração de texto\n",
    "def gerar_texto(inicial='o', tamanho=50):\n",
    "    modelo.eval()\n",
    "    input = torch.tensor([char2idx[inicial]], dtype=torch.long)\n",
    "    hidden = modelo.init_hidden()\n",
    "    resultado = [inicial]\n",
    "\n",
    "    for _ in range(tamanho):\n",
    "        out, hidden = modelo(input, hidden)\n",
    "        prob = torch.softmax(out[-1], dim=0).data\n",
    "        idx = torch.multinomial(prob, 1)[0]\n",
    "        char = idx2char[idx.item()]\n",
    "        resultado.append(char)\n",
    "        input = torch.tensor([idx])\n",
    "\n",
    "    return ''.join(resultado)\n",
    "\n",
    "print(gerar_texto(\"t\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tokenização e vocabulário\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "# Função de pré-processamento\n",
    "def process(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# Collate para batch\n",
    "def collate_batch(batch):\n",
    "    label_map = {\"pos\": 1, \"neg\": 0}\n",
    "    texts, labels = [], []\n",
    "    for label, text in batch:\n",
    "        tensor = torch.tensor(process(text), dtype=torch.long)\n",
    "        texts.append(tensor)\n",
    "        labels.append(label_map[label])\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    return padded, torch.tensor(labels)\n",
    "\n",
    "# DataLoader\n",
    "train_iter = IMDB(split='train')\n",
    "train_dataloader = DataLoader(list(train_iter), batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# Modelo LSTM\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.fc(hn[-1])\n",
    "        return torch.sigmoid(out).squeeze()\n",
    "\n",
    "# Treino\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentLSTM(len(vocab), embed_dim=64, hidden_dim=128).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_dataloader:\n",
    "        texts, labels = texts.to(device), labels.float().to(device)\n",
    "        preds = model(texts)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
